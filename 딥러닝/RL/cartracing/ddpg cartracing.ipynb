{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa4a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "state_size = 12*4\n",
    "action_size = 2\n",
    "\n",
    "load_model = False\n",
    "train_mode = True\n",
    "\n",
    "batch_size = 128\n",
    "mem_maxlen = 30000\n",
    "# BEFORE : 10000\n",
    "discount_factor = 0.9\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 5e-4\n",
    "tau = 1e-3\n",
    "\n",
    "mu = 0\n",
    "theta = 1e-3\n",
    "sigma = 2e-3\n",
    "\n",
    "run_step = 50000 if train_mode else 0\n",
    "save_step = 10000\n",
    "test_step = 10000\n",
    "train_start_step = 5000\n",
    "\n",
    "# 30만, 6만, 5만\n",
    "\n",
    "print_interval = 10\n",
    "save_interval = 100\n",
    "\n",
    "game = \"Kart1.exe\"\n",
    "file_path = f\"./{game}\"\n",
    "save_path = f\"./\"\n",
    "load_path = f\"./\"\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 128)\n",
    "        self.mu = torch.nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.mu(x))\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128+action_size, 128)\n",
    "        self.q = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.cat((x, action), dim=-1)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.q(x)\n",
    "\n",
    "class DDPGAgent():\n",
    "    def __init__(self):\n",
    "        self.actor = Actor().to(device)\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic = Critic().to(device)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.memory = deque(maxlen=mem_maxlen)\n",
    "        self.writer = SummaryWriter(save_path)\n",
    "\n",
    "        if load_model == True:\n",
    "            print(f\"... Load Model from {load_path}/ckpt ...\")\n",
    "            checkpoint = torch.load(load_path+'/ckpt', map_location=device)\n",
    "            self.actor.load_state_dict(checkpoint[\"actor\"])\n",
    "            self.target_actor.load_state_dict(checkpoint[\"actor\"])\n",
    "            self.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer\"])\n",
    "            self.critic.load_state_dict(checkpoint[\"critic\"])\n",
    "            self.target_critic.load_state_dict(checkpoint[\"critic\"])\n",
    "            self.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer\"])\n",
    "\n",
    "    def get_action(self, state, training=True):\n",
    "        \n",
    "        epsilon = 0.1 if training else 0.0\n",
    "        self.actor.train(training)\n",
    "\n",
    "        action = self.actor(torch.FloatTensor(state).to(device)).cpu().detach().numpy()\n",
    "        if training and np.random.rand() < epsilon:\n",
    "            random_value = np.random.uniform(-1, 1)\n",
    "            random_value2 = np.random.uniform(-1, 1)\n",
    "            action = np.array([[random_value, random_value2]], dtype=np.float32)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    # 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        state = np.stack([b[0] for b in batch], axis=0)\n",
    "        action = np.stack([b[1] for b in batch], axis=0)\n",
    "        reward = np.stack([b[2] for b in batch], axis=0)\n",
    "        next_state = np.stack([b[3] for b in batch], axis=0)\n",
    "        done = np.stack([b[4] for b in batch], axis=0)\n",
    "    \n",
    "        state, action, reward, next_state = map(\n",
    "            lambda x: torch.FloatTensor(x).to(device),\n",
    "            [state, action, reward, next_state]\n",
    "        )\n",
    "    \n",
    "        # Critic 업데이트\n",
    "        next_actions = self.target_actor(next_state)\n",
    "        next_q = self.target_critic(next_state, next_actions)\n",
    "        reward = torch.FloatTensor(np.stack([b[2] for b in batch], axis=0)).to(device)\n",
    "        done = torch.FloatTensor(np.stack([b[4] for b in batch], axis=0)).to(device)\n",
    "        reward = reward.view(-1, 1, 1)\n",
    "        done = done.view(-1, 1, 1)\n",
    "\n",
    "        target_q = reward + (1 - done) * discount_factor * next_q\n",
    "        q = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(target_q, q)\n",
    "    \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "    \n",
    "        # Actor 업데이트\n",
    "        action_pred = self.actor(state)\n",
    "        actor_loss = -self.critic(state, action_pred).mean()\n",
    "    \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "    \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    # 소프트 타겟 업데이트를 위한 함수\n",
    "    def soft_update_target(self):\n",
    "        for target_param, local_param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        for target_param, local_param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    # 네트워크 모델 저장\n",
    "    def save_model(self):\n",
    "            print(f\"... Save Model to {save_path}/ckpt ...\")\n",
    "            torch.save({\n",
    "            \"actor\" : self.actor.state_dict(),\n",
    "            \"actor_optimizer\" : self.actor_optimizer.state_dict(),\n",
    "            \"critic\" : self.critic.state_dict(),\n",
    "            \"critic_optimizer\" : self.critic_optimizer.state_dict(),\n",
    "        }, save_path+'/ckpt')\n",
    "\n",
    "    # 학습 기록\n",
    "    def write_summray(self, score, actor_loss, critic_loss, step):\n",
    "        self.writer.add_scalar(\"run/score\", score, step)\n",
    "        self.writer.add_scalar(\"model/actor_loss\", actor_loss, step)\n",
    "        self.writer.add_scalar(\"model/critic_loss\", critic_loss, step)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    engine_configuration_channel = EngineConfigurationChannel()\n",
    "    env = UnityEnvironment(file_name=file_path,\n",
    "                           side_channels=[engine_configuration_channel],\n",
    "                           worker_id=213)\n",
    "    env.reset()\n",
    "    \n",
    "    behavior_name = list(env.behavior_specs.keys())[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "    dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "    agent = DDPGAgent()\n",
    "\n",
    "    actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0\n",
    "    \n",
    "    for step in range(run_step + test_step):\n",
    "        if step == run_step:\n",
    "            if train_mode:\n",
    "                agent.save_model()\n",
    "            print(\"TEST START\")\n",
    "            train_mode = False\n",
    "            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "        \n",
    "        current_state = dec.obs[0]\n",
    "        action = agent.get_action(current_state, train_mode)\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_continuous(action)\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "        \n",
    "        dec, term = env.get_steps(behavior_name)\n",
    "        done = any(term.values())\n",
    "        reward = term.reward if done else dec.reward\n",
    "        next_state = term.obs[0] if done else dec.obs[0]\n",
    "        score += reward\n",
    "\n",
    "        if train_mode:\n",
    "            agent.append_sample(current_state, action, reward, next_state, done)\n",
    "\n",
    "        if train_mode and step > max(batch_size, train_start_step):\n",
    "            actor_loss, critic_loss = agent.train_model()\n",
    "            actor_losses.append(actor_loss)\n",
    "            critic_losses.append(critic_loss)\n",
    "\n",
    "            # 타겟 네트워크 소프트 업데이트\n",
    "            agent.soft_update_target()\n",
    "\n",
    "        if done:\n",
    "            episode += 1\n",
    "            scores.append(score)\n",
    "            score = 0\n",
    "\n",
    "            # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록\n",
    "            if episode % print_interval == 0:\n",
    "                mean_score = np.mean(scores) if scores else 0.0\n",
    "                mean_actor_loss = np.mean(actor_losses) if actor_losses else 0.0\n",
    "                mean_critic_loss = np.mean(critic_losses) if critic_losses else 0.0\n",
    "                agent.write_summray(mean_score, mean_actor_loss, mean_critic_loss, step)\n",
    "                actor_losses, critic_losses, scores = [], [], []\n",
    "\n",
    "                print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "                      f\"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}\")\n",
    "\n",
    "            # 네트워크 모델 저장\n",
    "            if train_mode and episode % save_interval == 0:\n",
    "                agent.save_model()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea6c7e-8711-4657-a69c-ec329ea79d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe66f6-12fb-4730-8ddb-ec79b09e88cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
